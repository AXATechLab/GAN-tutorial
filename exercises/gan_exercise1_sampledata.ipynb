{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest GAN ever\n",
    "\n",
    "Welcome to the first exercise in GANs.\n",
    "\n",
    "Remember than the **G** stands for _Generative_ network. So, we are interested in a generator. We want a network that can _create_ more data, according to some pre-defined distribution. However, this pre-defined distribution cannot be described analitically, only at a hight level, e.g. \"My distribution is _images of cats_\". So the _Generative_ network will have to learn this distribution from real data.\n",
    "\n",
    "Before we dive into images, let's start with something much simpler: a **two**-dimensional distribution. In other words, \"I want a network that can generate _points which lie on a circle_.\" Or _points which lie on a parabola_.\n",
    "\n",
    "Let's see how we can achieve this with a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need the GPU, make this a markdown cell, or comment the following lines\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The real data\n",
    "\n",
    "First, let's have a look at the real data, the one that we try to model. This consists of _2D points_, whose distribution is a parabola. Unlike pictures of cats, we can quickly \"acquire\" some of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(n=10000, scale=100):\n",
    "    \"\"\"Produce a numpy array of n 2D real samples\"\"\"\n",
    "    x = np.random.random_sample(size=(n,))\n",
    "    x = scale * (x - 0.5)\n",
    "    y = x ** 2 + 10\n",
    "\n",
    "    data = np.c_[x, y]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = sample_data(200)\n",
    "plt.plot(real[:, 0], real[:, 1], 'bo', markersize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The noise\n",
    "\n",
    "Our network will have to learn that dependency between the `x` and the `y` dimensions. However, it cannot do so from nothing. It can, however, learn a mapping.\n",
    "\n",
    "So the generator will take as input a noise vector `Z` and will transform it to look like the _real_ distribution above.\n",
    "\n",
    "Let's have a look at the noise vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(n=10000, scale=100):\n",
    "    \"\"\"Produce a numpy array of n 2D random samples\"\"\"\n",
    "    x = np.random.random_sample(size=(n,))\n",
    "    x = scale * (x - 0.5)\n",
    "\n",
    "    y = np.random.random_sample(size=(n,))\n",
    "    y = scale * (y - 0.5)\n",
    "\n",
    "    data = np.c_[x, y]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = sample_Z(200)\n",
    "plt.plot(noise[:, 0], noise[:, 1], 'ro', markersize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you noticed, the function for sampling the noise is very similar to the one for sampling real data. In this case, the noise vector is initialized truly randomly. But what if it wasn't?\n",
    "\n",
    "At a later stage, you can come back and modify the `sample_Z` function, in order to have a differently initialized vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *G* stands for _Generator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Build up generator network. \n",
    "\n",
    "**Input:** a noise vector `Z`. \n",
    " - The dimension of z could be smaller or the same with the label data. Theoretically the z could also be bigger than the label data, but then the generator is less meaningful since people use generator to map the low dimension data to high dimenstion, not the way around.\n",
    " - in our case, it is difficult to have a z of dimension much lower than 2 :). So the noise vector will also be of shape `[batch_size, 2]`\n",
    "\n",
    "**Output:** sample data that should be the same dimension with label data (in this case, 2).\n",
    "\n",
    "**Architecture:** you can build 2 or 3 layers in generator. \n",
    " - There is no standard how many layers we should use. More layers can give better result, but also have the risk to overfit. Also, more layers mean more memory needed. In practial tasks, you have to try by yourself. \n",
    " - For each layer, you can use fully connected layer or convolutional layer. You can call the function `tf.layers.dense` to build up one fully connected layer in tensorflow. Here is a detailed explanation and arguments for `tf.layers.dense` : https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
    " - You should give the inputs and the units to the function, and use `tf.nn.relu` or `tf.nn.leaky_relu` as activation function. But of course you can skip inputing the activation function and have a look what will happen in the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(Z, hsize=[16, 16], reuse=False):\n",
    "    with tf.variable_scope(\"GAN/Generator\", reuse=reuse):\n",
    "        h1 = tf.layers.dense(Z, hsize[0], activation=tf.nn.leaky_relu) # layer 1\n",
    "        h2 = # complete the rest of the code\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *A* stands for _Adversarial_\n",
    "\n",
    "Who is the adversary? A _Discriminator_ network. It's task is to say whether the data produced by the _Generator_ is real or fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Build up discriminator network. \n",
    "\n",
    "**Input:** sample data or real data (ground truth). Input dimension: `[batch_size, 2]`\n",
    "\n",
    "**Output:** [batch_size,1] dimension scalar. The discriminator should output one probability scalar to tell real or fake. If you feed in a batch, then the output is `[batch_size, 1]`.\n",
    "\n",
    "**Architecture:** 3 or 4 layers. You can still use fully connected layer as what you have in generator. But notice that, the output of last layer must be `[batch_size, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X, hsize=[16, 16], reuse=False):\n",
    "    with tf.variable_scope(\"GAN/Discriminator\", reuse=reuse):\n",
    "        h1 = tf.layers.dense(X, hsize[0], activation=tf.nn.leaky_relu) # layer 1 \n",
    "        h2 = # complete the rest of the code\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *N* stands for ... (trained) _Network_\n",
    "\n",
    "In other words, to have a useful network, we need to train it. In order to train it, we need some inputs (real data X and noise vector Z) and some loss(es). And this is where the fun starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,[None,2])\n",
    "Z = tf.placeholder(tf.float32,[None,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the generator and discriminator :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sample = generator(Z)\n",
    "real_logits = discriminator(X)\n",
    "fake_logits = discriminator(G_sample, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: complete the generator loss and discriminator loss according to the objective function.  \n",
    "\n",
    "Note that: when discriminator is updated, the generator is fixed; And when generator is updated, the discriminator is fixed. This is important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator loss: \n",
    "\n",
    "![Discriminator loss image](../images/discriminator_loss.png)\n",
    "\n",
    "In other words, the discriminator tries to output `1` for the real images and `0` for the fake ones. The functions `tf.ones_like` and `tf.zeros_like` can come in useful.\n",
    "\n",
    "You can use the function `tf.nn.sigmoid_cross_entropy_with_logits` to compute the cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_loss = # complete the calculation of the loss as the sum of two losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Loss:\n",
    "\n",
    "![Generator loss image](../images/generator_loss.png)\n",
    "\n",
    "Or, in natural language, the generator tries to make the output of the discriminator be `1` for the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loss = # complete the calculation of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the trainable variables and build the optimizer:\n",
    "\n",
    "Note: there are many different optimizer in tensorflow such as AdadeltaOptimizer, AdamOptimizer, GradientDescentOptimizer. You can play with different optimizer and compare the results. But please notice that different optimizer might require different arguments. Please google it first to get the details.\n",
    "\n",
    "Having 2 different optimizers for G and D means that each is kept constant when the other updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Generator\")\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GAN/Discriminator\")\n",
    "\n",
    "gen_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(gen_loss, var_list=gen_vars) # G Train step\n",
    "disc_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(disc_loss, var_list=disc_vars) # D Train step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10001):\n",
    "    step += 1\n",
    "    X_batch = sample_data(n=batch_size)\n",
    "    Z_batch = sample_Z(n=batch_size)\n",
    "\n",
    "    # Train discriminator \n",
    "    _, dloss = sess.run([disc_step, disc_loss], feed_dict={X: X_batch, Z: Z_batch})\n",
    "\n",
    "    # Train generator\n",
    "    _, gloss, np_G_sample = sess.run([gen_step, gen_loss, G_sample], feed_dict={Z: Z_batch})\n",
    "\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\" % (step, dloss, gloss))\n",
    "        plt.clf()\n",
    "        plt.plot(X_batch[:,0], X_batch[:,1], 'bo', \n",
    "                 Z_batch[:,0], Z_batch[:, 1], 'k.',\n",
    "                 np_G_sample[:,0], np_G_sample[:,1], 'ro',\n",
    "                 markersize=2)\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune to your liking\n",
    "\n",
    "So, now you have seen the GAN training. If you're not happy with the above result, feel free to run the above cell once more, to train the GAN further.\n",
    "\n",
    "Or you can go back and modify the `sample_Z` function, to start with a better noise vector. Note that if you do this, you'll probably have to re-start the kernel, or your GAN will continue training.\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
